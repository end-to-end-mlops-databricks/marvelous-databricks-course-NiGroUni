# This is a Databricks asset bundle definition for marvelous-databricks-course-NiGroUni.
# The Databricks extension requires databricks.yml configuration file.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.

bundle:
  name: marvelous-databricks-course-NiGroUni
  # cluster_id: 1014-134439-1sgb0b2m

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://adb-6130442328907134.14.azuredatabricks.net

  ## Optionally, there could be 'staging' or 'prod' targets here.
  #
  # prod:
  #   workspace:
  #     host: https://adb-6130442328907134.14.azuredatabricks.net
#   prod:
#     # default: true
#     workspace:
#       host: https://adb-6130442328907134.14.azuredatabricks.
#       root_path: ${var.root_path}

# artifacts:
#   default:
#     type: whl
#     build: python -m build
#     path: .

include:
  - bundle_monitoring.yml

variables:
  root_path:
    description: root_path for the target
    default: /Workspace/Users/nico.grosskreuz@unicef.de/.bundle/${bundle.name}/${bundle.target}/files
  git_sha:
    description: git_sha
    default: abcd
  schedule_pause_status:
    description: schedule pause status
    default: UNPAUSED

resources:
  jobs:
    power-consumption:
      name: power-consumption-workflow-test-demo
      schedule:
        quartz_cron_expression: "0 0 6 ? * MON"
        timezone_id: "Europe/Amsterdam"
        pause_status: ${var.schedule_pause_status}
      tags:
        env: dev
        purpose: ds
      job_clusters:
        - job_cluster_key: "power-consumption-cluster"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            spark_env_vars:
              LOG_AZURE_ACCOUNT_URL: "{{secrets/uni-data-keyvault/adls-account-url-deltalake}}"
              LOG_AZURE_SAS: "{{secrets/uni-data-keyvault/adls-logs-general-SAStoken}}"
              PRODUCTION_LANE: dev
            azure_attributes:
              first_on_demand: 1
              availability: SPOT_WITH_FALLBACK_AZURE # if no spot vm is found use normal azure vm
              spot_bid_max_price: 100 # bit up to 100 percent of normal price for spot vms
            data_security_mode: "SINGLE_USER"
            node_type_id: "Standard_E4d_v4"
            # driver_node_type_id: "Standard_E4d_v4"
            num_workers: 1
            policy_id: 30637689B5001B83
            custom_tags:
              purpose: ds
              env: dev

      tasks:
        - task_key: "preprocessing"
          job_cluster_key: "power-consumption-cluster"
          # existing_cluster_id: 1014-134439-1sgb0b2m
          spark_python_task:
            python_file: "notebooks/week5/01.preprocess.py"
            parameters:
              - "--root_path"
              - ${var.root_path}
          libraries:
           - whl: ./dist/*.whl
        - task_key: if_refreshed
          condition_task:
            op: "EQUAL_TO"
            left: "{{tasks.preprocessing.values.refreshed}}"
            right: "1"
          depends_on: 
            - task_key: "preprocessing"
        - task_key: "train_model"
          depends_on: 
            - task_key: "if_refreshed"
              outcome: "true"
          job_cluster_key: "power-consumption-cluster"
          # existing_cluster_id: 1014-134439-1sgb0b2m
          spark_python_task:
            python_file: "notebooks/week5/02.train_model.py"
            parameters:
              - "--root_path"
              - ${var.root_path}
              - "--git_sha"
              - ${var.git_sha}
              - "--job_run_id"
              - "{{job.id}}"
          libraries: 
            - whl: ./dist/*.whl
        - task_key: "evaluate_model"
          depends_on: 
            - task_key: "train_model"
          job_cluster_key: "power-consumption-cluster"
          # existing_cluster_id: 1014-134439-1sgb0b2m
          spark_python_task:
            python_file: "notebooks/week5/03.evaluate_model.py"
            parameters:
              - "--root_path"
              - ${var.root_path}
              - "--new_model_uri"
              - "{{tasks.train_model.values.new_model_uri}}"
              - "--job_run_id"
              - "{{job.id}}"
              - "--git_sha"
              - ${var.git_sha}
          libraries: 
            - whl: ./dist/*.whl
        - task_key: model_update
          condition_task:
            op: "EQUAL_TO"
            left: "{{tasks.evaluate_model.values.model_update}}"
            right: "1"
          depends_on: 
            - task_key: "evaluate_model"
        - task_key: "deploy_model"
          depends_on: 
            - task_key: "model_update"
              outcome: "true"
          job_cluster_key: "power-consumption-cluster"
          # existing_cluster_id: 1014-134439-1sgb0b2m
          spark_python_task:
            python_file: "notebooks/week5/04.deploy_model.py"
            parameters:
              - "--root_path"
              - ${var.root_path}              
          libraries: 
            - whl: ./dist/*.whl
